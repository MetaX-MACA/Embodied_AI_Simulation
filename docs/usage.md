# Usage
Please refer to [install.md](install.md) for environment setup.

The pipeline of this project consists of two main modules:  
1. Skill task and simulation configuration generation based on LLM,  
2. Environment creation in MuJoCo based on the generated tasks, followed by skill learning using reinforcement learning.

## LLM model preparation

### Use Qwen2.5-Coder-14B-Instruct locally
In our project, we use the [Qwen2.5-Coder-14B-Instruct](https://modelscope.cn/models/Qwen/Qwen2.5-Coder-14B-Instruct/) model for local deployment, accessed via the OpenAI API format. It is used for task generation, simulation environment configuration, subtask decomposition, and reward function code generation.

You can use the following command to download the `Qwen2.5-Coder-14B-Instruct` model weights from modelscope, which will require approximately 28GB of storage space.
```shell
pip install modelscope
modelscope download --model Qwen/Qwen2.5-Coder-14B-Instruct
```

After the weights are downloaded, please modify the `gpt/run_qwen.sh` file to set the `--model` argument to the directory where the `Qwen2.5-Coder-14B-Instruct` weights are located.

### Use other LLMs online
You can also use other LLM models that support the OpenAI API interface online (such as ChatGPT, DeepSeek, etc.). After obtaining the corresponding modelâ€™s `api_key` and `api_url`, you need to modify `gpt/qwen_query.py` and replace them with your own `api_key` and `api_url`.

``` python
client = OpenAI(
    api_key=your_api_key,
    base_url=your_api_url, 
)
```

Then, you need to modify `run_kitchen.py`, set the variable `model_dict` to your LLM model name.

``` python
model_dict = {
        "task_generation": "your LLM model name",
        "reward": "your LLM model name",
        "yaml": "your LLM model name",
        "joint": "your LLM model name",
    }
```

**Note** If you use other LLM to generate tasks, the **responses may not match** the post-processing and parsing code, which could lead to task execution failures. 

In such cases, you may need to review the post-processing code and make necessary adjustments to ensure compatibility with the new LLM's responses.

## Running LLM model
Starting the API server using vLLM to run the `Qwen2.5-Coder-14B-Instruct` model . It may cost about 40GB GPU memory.

```
bash gpt/run_qwen.sh
```

## Task generation

Use the following command to generate multi task configurations, simulation environment configurations, subtask decomposition, and reward function code.

This step requires either starting a local LLM API server or using a third-party LLM API.

``` shell
python run_kitchen.py
```

You can add argument `--execute` to random choose one task file for execution after task generation.
``` shell
python run_kitchen.py --execute
```

## Task Execution
Use the following command to execute task, this step need the task yaml file generated by previout step `run_kitchen.py`.

``` shell
python execute.py --config path_to_task_yaml_file_generated_by_run_kitchen
```

If you are unable to use an LLM model or have trouble during the task generation phase, you can use the sample task configuration files we provide to run MuJoCo simulation and reinforcement learning tasks. The sample task configuration files are in `data/sample_kitchen_tasks`.

``` shell
python execute.py --config data/sample_kitchen_tasks/Open_Microwave_Door_The_robotic_arm_will_open_the_microwave_door.yaml
```

## Usage Note

### In task generation

- Due to the randomness of LLM responses, parsing the responses may fail. In such cases, you can either adjust the post-processing parsing code or simply rerun the script `run_kitchen.py`.

### In task execution
- MuJoCo is configured by default to use CPU-based headless mode for simulation rendering. Due to limited rendering precision in this mode, some object surfaces may appear with broken or disconnected triangles. If your server supports a graphical desktop environment, you can add the `--gui` flag when running the script to enable visual rendering. This would improve the rendering quality.

- Due to the inherent randomness in the motion paths planned by OMPL, unexpected collisions may occur during the robotic arm's movement, leading to task execution that does not meet expectations. In such cases, rerunning the script is recommended.

- Since asset positions are randomly generated by the LLM within the desktop workspace, the robotic arm may fail to reach the target due to limited arm length or joint constraints. This can lead to OMPL motion planning failure or reinforcement learning task failure. In such cases, you can choose to run the sample configuration files we provide.

In future work, we plan to replace the current robotic arm asset with one that has a mobile base to avoid such issues.